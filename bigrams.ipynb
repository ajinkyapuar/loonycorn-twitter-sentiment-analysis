{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "tweets = ['New York is a great city',\n",
    "          'I love my new shirt',\n",
    "          'The New York Times is a great paper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('New', 'York')\n('York', 'is')\n('is', 'a')\n('a', 'great')\n('great', 'city')\n"
     ]
    }
   ],
   "source": [
    "for bg in bigrams(tweets[0].split()):\n",
    "    print bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for tweet in tweets:\n",
    "    all_words.extend(tweet.split()) \n",
    "    \n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "bigram_finder = BigramCollocationFinder.from_words(all_words)\n",
    "\n",
    "bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'love'), ('New', 'York'), ('a', 'great')]\n"
     ]
    }
   ],
   "source": [
    "print bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'city', 'great', 'love', 'shirt', 'I', 'is', 'Times', 'paper', 'York', 'new', 'New', 'The', 'my']\n['a', 'city', 'great', 'love', 'shirt', 'I', 'is', 'Times', 'paper', 'York', 'new', 'New', 'The', 'my', ('I', 'love'), ('New', 'York'), ('a', 'great')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "word_list = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = word_list.keys()\n",
    "print word_features\n",
    "\n",
    "word_features.extend(bigrams)\n",
    "\n",
    "print word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet.split())\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % str(word)] = (set(word).issubset(tweet_words))\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contains(is)': False, 'contains(a)': True, \"contains(('I', 'love'))\": False, 'contains(great)': False, \"contains(('New', 'York'))\": True, 'contains(my)': False, 'contains(New)': False, 'contains(The)': False, 'contains(York)': False, 'contains(shirt)': False, 'contains(new)': False, 'contains(love)': False, 'contains(paper)': False, 'contains(city)': False, 'contains(I)': False, 'contains(Times)': False, \"contains(('a', 'great'))\": True}\n"
     ]
    }
   ],
   "source": [
    "trainingFeatures = nltk.classify.apply_features(extract_features, tweets)\n",
    "\n",
    "print trainingFeatures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}